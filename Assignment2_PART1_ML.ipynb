{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From text to a numerical representation\n",
    "Typically, Machine Learning algorithms require a vectorial representation for their data. In the IR module, we saw how text can be pre-processed, tokenized, and converted to vector representations. In this tutorial we will be re-using these ideas to create vector representations of documents for the purpose of training classifiers.\n",
    "\n",
    "We will be using the `scikit-learn` Python library, which provides functionallity for text pre-processing and training and using classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some configurations for notebook and importing modules\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(6490)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we be using the `newsgroup dataset`, in which documents belong to one of nine categories.\n",
    "\n",
    "We beging by loading the dataset into a `pandas` Dataframe, which is a two-dimensional labeled data-structure (similar to a table), in which columns represent attributes and rows represent data instances. \n",
    "\n",
    "In the resulting `dataset` Dataframe, each row is a document, and the columns represent the id, category and text, of the given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import read_as_df\n",
    "from prepros import preprocessor\n",
    "import os.path\n",
    "\n",
    "path_to_dataset = os.path.join('question_1_data', 'newsgroups')\n",
    "\n",
    "dataset = read_as_df(path_to_dataset)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the preprocessing function in the file `prepros.py`, to pre-process each document in the dataset. \n",
    "This will add a new column to the previously created Dataframe.\n",
    "\n",
    "**NB** This next block might take a while to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tokens'] = dataset['text'].apply(preprocessor)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CountVectorizer\n",
    "\n",
    "We use scikit-learn's [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for vectorizing the tokens.\n",
    "\n",
    "CountVectorizer itself can remove stop_words, convert text into lowercase tokens.\n",
    "However, we will not use these options since we have already built a more sophisticated tokenizer, which can stem tokens (which not readily available in scikit-learn). \n",
    "\n",
    "To use our tokenizer, we define CountVectorizer with options `tokenizer = lambda x: x`, which means we are asking CountVectorizer to apply the identity function, as we already have a list of tokens available. \n",
    "\n",
    "Note that we can set `binary=True` option to use a boolean representation. Setting it to False outputs a term-frequency representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(lowercase = False, \n",
    "                                     tokenizer = lambda x: x, # because we already have tokens available\n",
    "                                     stop_words = None, ## stop words removal already done from NLTK\n",
    "                                     max_features = 5000, ## pick top 5K words by frequency\n",
    "                                     ngram_range = (1, 1), ## we want unigrams for now\n",
    "                                     binary = False) ## we want frequency count features\n",
    "text_vec = bow_vectorizer.fit_transform(dataset.tokens)\n",
    "print(text_vec[0, ]) ## see the features indices that are set to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorial representation represents each document as a vector, in which each dimension corresponds to a word/token in the vocabulary build from the entire dataset/corpus. \n",
    "\n",
    "For example, one of the line in the output states:\n",
    "\n",
    "`(0, 3863)\t6`\n",
    "\n",
    "which means that the first document has 6 occurrences of the $3863^{th}$ feature (word).\n",
    "\n",
    "Let's see what word corresponds to that index, and what are the first 100 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bow_vectorizer.get_feature_names()[3863])\n",
    "## list of feature names (they are just tokens here)\n",
    "print(bow_vectorizer.get_feature_names()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sparsity of the feature space\n",
    "The feature space is sparse, and as a result CountVectorizer represents documents them using a [sparse matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html), instead of a [dense matrix](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.html)\n",
    "(if each document is a vector, a collection of documents corresponds to a matrix).\n",
    "\n",
    "To verify this, we can check how many features are enabled in the matrix corresponding to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} values are set, out of a maximum of {} = {:.2f}%'.format(\n",
    "    text_vec.nnz, dataset.shape[0] * 10000, 1.0 * text_vec.nnz / (dataset.shape[0] * 10000) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, less than 1% of the matrix elements are set, so representing the data with dense matrix would be an inefficient usage of memory. \n",
    "Hence, `scikit-learn`'s [Count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) internally uses sparse representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 [7 pts]\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "### Sections 1.2-1.5 of this notebook each have some tasks for you to complete, these are marked in blue. Completeing these tasks is question 1 of the ML assignment, and you will need to submit this notebook along with your NLP Assignment.ipynb notebook.\n",
    "</span>\n",
    "\n",
    "# 1. Building a classifier\n",
    "\n",
    "## 1.1 Naive Bayes Classifier\n",
    "\n",
    "Having represented documents in the Vector Space Model, we can now start building a Naive Bayes textual classifier.\n",
    "\n",
    "To do so, we need will need counts of the term occurrences when computing the $ \\hat P(t | c)$.\n",
    "This amounts to constructing a term-frequency vectorial representation.\n",
    "\n",
    "Therefore, the [Naive bayes classifier](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) implementation available in `scikit-learn` requires the document collection to be in a vectorial representation prior to training Naive Bayes.\n",
    "\n",
    "Next, we split the dataset into a training set (75% of the dataset) and a testing set (25% of the dataset). \n",
    "We train a Naive Bayes Classifier on the training set, and we perform the predictions on the test set.\n",
    "\n",
    "**NB** that we are using `LabelEncoder` here to encode labels/classes of documents as numbers. \n",
    "The 9 classes will be mapped into numbers from 0 to 8 using this label encoder. \n",
    "We require this to render the dataset compatible with `scikit-learn` and the plotting libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "msk = np.random.rand(len(dataset)) < 0.75\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_X = text_vec[msk]\n",
    "test_X = text_vec[~msk]\n",
    "\n",
    "y = le.fit_transform(dataset.category)\n",
    "train_y = y[msk]\n",
    "test_y = y[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the classifier using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier =  MultinomialNB()\n",
    "classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, we use the `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_bow = classifier.predict(test_X)\n",
    "to_print = [le.inverse_transform([pred])[0] for pred in preds_bow ]\n",
    "print(to_print[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Evaluating the prediction performance [1 pts]\n",
    "To evaluate how well the classifier performed, we compute the confusion matrix, as well as the overall accuracy, and the per-class precision, recall and F1 measure. \n",
    "\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "### You will need to fill in the code that computes these measure, but note that these are all implemented in the scikit_learn library, and you should make use of this. [1 pts]\n",
    "</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_metrics(y, pred_y):\n",
    "    # Correctly assign thse variables\n",
    "    raise NotImplementedError\n",
    "    confusion = None\n",
    "    acc = None\n",
    "    precisions, recalls, f1_scores, _ = None\n",
    "\n",
    "    print(\"accuracy = {}\".format(acc))\n",
    "\n",
    "    print(\"{:>25} {:>4} {:>4} {:>4}\".format(\"\", \"prec\", \"rec\", \"F1\"))\n",
    "    for (idx, scores) in enumerate(zip(precisions, recalls, f1_scores)):\n",
    "        print(\"{:>25} {:.2f} {:.2f} {:.2f}\".format(\n",
    "            le.inverse_transform([idx])[0], scores[0], scores[1], scores[2]\n",
    "    ))\n",
    "\n",
    "    print('confusion matrix:\\n{}'.format( confusion) )\n",
    "    \n",
    "    return acc\n",
    "\n",
    "acc_bow = print_metrics(test_y, preds_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Improving the performance with different feature representations [2 pts]\n",
    "\n",
    "We attempt to improve the classifier's performance using other features.\n",
    "We start with the boolean features: whenever a feature (token) appears in a document we mark a value of 1 instead of number of occurrence of that token.\n",
    "\n",
    "We can pipeline vectorization, and classifier in scikit learn. Refer [this documentation](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "train_X = dataset.tokens[msk]\n",
    "test_X = dataset.tokens[~msk]\n",
    "y = le.fit_transform(dataset.category)\n",
    "train_y = y[msk]\n",
    "test_y = y[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will use CountVectorizer, but the difference here is `binary = True` argument, which tells CountVectorizer to use binary features instead of term frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "### You will need to fill in the code to fit the pipeline model on the training data and create predictions for the test data. [1 pts]\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_vectorizer = CountVectorizer(lowercase = False, \n",
    "                                     tokenizer = lambda x: x, # because we already have tokens available\n",
    "                                     stop_words = None, ## stop words removal already done from NLTK\n",
    "                                     max_features = 5000, ## pick top 5K words by frequency\n",
    "                                     ngram_range = (1, 1), ## we want unigrams now\n",
    "                                     binary = True) ## Now it is Binary\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow',  bin_vectorizer),\n",
    "    ('naive-bayes',  MultinomialNB()) ])\n",
    "\n",
    "# Train the pipeline and create predictions for the test set.\n",
    "raise NotImplementedError\n",
    "preds_bin = None\n",
    "\n",
    "acc_bin = print_metrics(test_y, preds_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "### Did changing the model to use boolean features instead of term-frequency increase or decrease the model's performance? Explain why the performance did or did not change. [1 pts]\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> YOUR ANSWER HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Improving the performance with  TFIDF [2 pts]\n",
    "TF-IDF reflects how important a token/term is to a document, with respect to the entire collection of documents. \n",
    "\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "\n",
    "### To build a pipeline with TF-IDF representation, you will need to add a [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) after the bag-of-words vectorizer. [1 pts]\n",
    "</span>\n",
    "\n",
    "It means that we are transforming the token counts using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Create the pipeline and train and evaluate the model.\n",
    "raise NotImplementedError\n",
    "pipeline = None\n",
    "preds_tfidf = None\n",
    "\n",
    "acc_tfidf = print_metrics(test_y, preds_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "### Did changing the model to use tf-idf features increase or decrease the model's performance? Explain why the performance did or did not change. [1 pts]\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> YOUR ANSWER HERE </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pd.DataFrame(\n",
    "    [('tf', acc_bow), ('binary', acc_bin), ('tfidf', acc_tfidf)], \n",
    "    columns = ['feature_rep', 'accuracy']\n",
    ").set_index('feature_rep')\n",
    "accuracies.plot.bar(ylim = (0.7, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.5 Further improving performance with Bigrams features [2 pts]\n",
    "Until now, we created feature representations using unigrams, i.e. taking one token as a feature. \n",
    "The main disadvantage of doing this is that we loss positional information in unigram feature representation. \n",
    "To address this, we can use n-gram as a features: we use sequences of n words to construct features.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "### You will need to train your best performing model, this time usinig both unigram and bigram features. [1 pts]\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Define the CountVectorizer to use bigrams as well as unigrams.\n",
    "raise NotImplementedError\n",
    "bigrams_bow_vectorizer = None\n",
    "pipeline = None\n",
    "preds_tfidf_bigrams = None\n",
    "\n",
    "acc_tfidf_bigrams = print_metrics(test_y, preds_tfidf_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "### Did changing the model to use bigram and unigram features increase or decrease the model's performance? Explain why the performance did or did not change. [1 pts]\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> YOUR ANSWER HERE </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pd.DataFrame(\n",
    "    [('unigrams', acc_tfidf),  ('bigrams', acc_tfidf_bigrams)], \n",
    "    columns = ['n-grams', 'accuracy']\n",
    ").set_index('n-grams')\n",
    "accuracies.plot.bar(ylim = (0.8, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what bigrams features look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(bigrams_bow_vectorizer.get_feature_names()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice sequences of two tokens used as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDITIONAL MATERIAL \n",
    "\n",
    "## Distance metrics and searching in the Vector Space Model\n",
    "\n",
    "### Distance metrics for a document\n",
    "\n",
    "In the previous tutorial, we used boolean logic on the set representation of features to perform exact document match. With vector representations introduced in this module, we can query for partial matches. \n",
    "\n",
    "The degree of matching can be quantified by similarity metrics. \n",
    "The similarity metrics can be computed from distance metrics, where the distance between documents are computed in vector spaces. There are two popular choices for distance metrics in this space:\n",
    "1. Cosine distance\n",
    "2. Euclidean distance\n",
    "\n",
    "To search for similar documents, we use the candidates that has minimum distance with the query's vector representation. We use the TFIDF vectorizer with unigrams representation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Pipeline([\n",
    "    ('bow',  bow_vectorizer),\n",
    "    ('tfidf',  TfidfTransformer())])\n",
    "vectorizer.fit(dataset.tokens)\n",
    "tfidf_vec = vectorizer.transform(dataset.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this vectorizer to create a vector representation of our previous two queries: `research seminar` and `scientific visualization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = preprocessor(\"research seminar\")\n",
    "query_vec1 = vectorizer.transform([tokens1])\n",
    "\n",
    "tokens2 = preprocessor(\"scientific visualization\")\n",
    "query_vec2 = vectorizer.transform([tokens2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the distance metrics are provided in the file `dist.py`. \n",
    "Have a look at the functions provided there.\n",
    "\n",
    "We can use the provided `dist` function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist import dist, search\n",
    "cosine_distance = dist(query_vec1.toarray().squeeze(), query_vec2.toarray().squeeze(), method = 'cosine')\n",
    "euclid_distance = dist(query_vec1.toarray().squeeze(), query_vec2.toarray().squeeze(), method = 'euclid')\n",
    "\n",
    "print('cosine distance = {}, euclid distance = {}'.format(cosine_distance, euclid_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and ranking\n",
    "\n",
    "We use the same distance measures to search for similar documents for a query text. \n",
    "The most similar documents in our collections are the ones that have lowest distance against the query string. \n",
    "We can also use the distance to rank the search results.\n",
    "\n",
    "Have a look at search function in `dist.py`. \n",
    "\n",
    "Top 5 matches with `research seminar`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get indexes of the most similar documents \n",
    "idxs1 = search(tfidf_vec.toarray().squeeze(),\n",
    "        query_vec1.toarray().squeeze(),\n",
    "        dist_measure = 'cosine'\n",
    ")\n",
    "\n",
    "## ranked top 5 search results for query 'research seminar'\n",
    "dataset.iloc[idxs1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 matches with `scientific visualization` based on euclidean distance measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs2 = search(tfidf_vec.toarray().squeeze(),\n",
    "        query_vec2.toarray().squeeze(),\n",
    "        dist_measure = 'euclid'\n",
    ")\n",
    "\n",
    "## ranked top 5 search results for query 'research seminar'\n",
    "dataset.iloc[idxs2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## AUC and ROC \n",
    "\n",
    "Another popular evaluation metric for evaluating per class performance of a classifier is the [Area under Curve (AUC) of the Receiver Operating Characteristics (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). \n",
    "\n",
    "The ROC curve plots the true positive rate (Sensitivity) against the false positive rate (Specificity) for different cut-off points. \n",
    "Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. \n",
    "\n",
    "AUC is the area under ROC curve. $ AUC \\in [0.5, 1]$ and a value of $0.5$ corresponds to a random classifiers. Higher is better.\n",
    "\n",
    "The block below demonstrates how we can use matplotlib (python plotting library) and scikit's evaluation metric functions to plot per class performance of our classifier. \n",
    "We will see the plot for our best performing classifier (i.e. TFIDF with bigrams feature representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bigram_bow',  bigrams_bow_vectorizer),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('naive-bayes',  MultinomialNB()) ])\n",
    "\n",
    "## Fit the data\n",
    "pipeline.fit(train_X, train_y)\n",
    "\n",
    "## This function plots the ROC curve\n",
    "def plot_roc(labels, probs, le):\n",
    "    colors = cycle(['aqua', 'red', 'green', 'blue', 'yellow', 'cyan', 'magenta', 'violet', 'purple', 'black', 'grey'])\n",
    "    fpr, tpr = dict(), dict()\n",
    "    roc_auc = dict()\n",
    "    for label in range(len(list(le.classes_))):\n",
    "        color = next(colors)\n",
    "        fpr[label], tpr[label], _ = roc_curve(test_y == label, probs[:, label])\n",
    "        roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "        plt.plot(fpr[label], tpr[label], color = color, lw=2,\n",
    "                 label ='ROC of {0} | auc = {1:0.2f}'\n",
    "                 ''.format(le.inverse_transform([label])[0], roc_auc[label]))\n",
    "    plt.xlim([0.0, 1.1])\n",
    "    plt.ylim([0.0, 1.1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.figure(figsize=(9, 6))  # the plot size you want\n",
    "\n",
    "## see accuracy and confusion matrix\n",
    "preds_tfidf_bigrams = pipeline.predict_proba(test_X)\n",
    "plot_roc(test_y, preds_tfidf_bigrams, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
